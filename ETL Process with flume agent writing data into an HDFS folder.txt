-- This focuses on using Spark Streaming and different sources and sinks to perform an ETL with IoT data from a Nexus phone


-- SETTING UP THE FLUME AGENT
-- download flume
wget http://archive.apache.org/dist/flume/1.9.0/apache-flume-1.9.0-bin.tar.gz 
 
-- unzip the file in the home directory
tar -xvf apache-flume-1.9.0-bin.tar.gz

-- add a path to the .bashrc file and source it
vi .bashrc
export PATH=$PATH:/home/mailtessflo16/apache-flume-1.9.0-bin/bin
source .bashrc

--replace the older version of Guava with GCP's version
rm /home/mailtessflo16/apache-flume-1.9.0-bin/lib/guava-11.0.2.jar 
cp /lib/hadoop/lib/guava-28.2-jre.jar /home/mailtessflo16/apache-flume-1.9.0-bin/lib/.

/* There are 5 json files, viz., 1-stand.json, 2-sit.json, 3-stairsdown.json, 4-bike.json, and 5-sit.json
upload and merge the json files in the home directory
*/

cat 1-stand.json 2-sit.json 3-stairsdown.json 4-bike.json 5-sit.json > data.json

--initiating the flume agent
mkdir flume
cd flume/
mkdir simple
cd simple/
sudo vi flume-simple.config

-- in the configuration file, the hdfs path, sink, and sources are updated and loaded here

-- Create "BigData" folder in hadoop and checkpoint folders in the home directory
hadoop fs -mkdir /BigData/.
mkdir chkpt
mkdir chkpt2
mkdir chkpt3
mkdir chkpt4

-- To confirm the files in the home directory and in hadoop
ls

-- Start the flume agent

flume-ng agent --conf /home/mailtessflo16/flume/simple/ -f /home/mailtessflo16/flume/simple/flume-simple.config -Dflume.root.logger=DEBUG,console -n agent

--verify the json files are in the hadoop directory
hadoop fs -ls /BigData



--SETTING UP THE SINK

wget https://archive.apache.org/dist/zookeeper/zookeeper-3.4.14/zookeeper-3.4.14.tar.gz
tar -xzf zookeeper-3.4.14.tar.gz
rm zookeeper-3.4.14.tar.gz
cd zookeeper-3.4.14/
cd conf/
cp zoo_sample.cfg zoo.cfg
vi zoo.cfg

/* change tmp to var (dataDir)
server.0=127.0.0.1:2888:3888
:wq
*/

sudo mkdir /var/zookeeper
sudo chown mailtessflo16:mailtessflo16  /var/zookeeper
sudo vi /var/zookeeper/myid
0
cd ..
bin/zkServer.sh start

cd ..
wget https://packages.confluent.io/archive/4.1/confluent-4.1.4-2.11.tar.gz
tar -xzf confluent-4.1.4-2.11.tar.gz 
rm confluent-4.1.4-2.11.tar.gz 
cd confluent-4.1.4/
sudo vi etc/kafka/server.properties

--broker.id = 0

sudo vi etc/kafka/zookeeper.properties
--tmp to var

bin/kafka-server-start etc/kafka/server.properties > /dev/null 2>&1 &

--Create the topic, partitions, and consumers
bin/kafka-topics --create --zookeeper localhost:2181 --partitions 5 --replication-factor 1 --topic idle
bin/kafka-topics --create --zookeeper localhost:2181 --partitions 5 --replication-factor 1 --topic active
bin/kafka-console-consumer --bootstrap-server localhost:9092 --topic idle
bin/kafka-console-consumer --bootstrap-server localhost:9092 --topic active



-- SPARK STREAMING
spark-shell --master local --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1

import org.apache.spark.sql._
import org.apache.spark.sql.types._ 
 
val userSchema = new StructType()
 .add("Arrival_Time", "string")
 .add("Device", "string")
 .add("gt", "string")

 
val nexus1 = spark.readStream.format("json")
 .schema(userSchema)
 .option("path", "hdfs:///BigData/").load()


 val nexus_key_val = nexus1.withColumn("key", lit(100))
 .select(col("key").cast("string"), concat(col("Arrival_Time"), lit("  "), col("Device"), lit(" "), col("gt")).alias("value")).filter(col("gt")==="stand")

 val nexus_key_val2 = nexus1.withColumn("key", lit(100))
 .select(col("key").cast("string"), concat(col("Arrival_Time"), lit("  "), col("Device"), lit(" "), col("gt")).alias("value")).filter(col("gt")==="sit")

 val nexus_key_val3 = nexus1.withColumn("key", lit(100))
 .select(col("key").cast("string"), concat(col("Arrival_Time"), lit("  "), col("Device"), lit(" "), col("gt")).alias("value")).filter(col("gt")==="bike")

 val nexus_key_val4 = nexus1.withColumn("key", lit(100))
 .select(col("key").cast("string"), concat(col("Arrival_Time"), lit("  "), col("Device"), lit(" "), col("gt")).alias("value")).filter(col("gt")==="stairsdown")

val stream = nexus_key_val.writeStream
 .format("kafka")
 .option("kafka.bootstrap.servers", "localhost:9092")
 .option("topic", "idle") .option("checkpointLocation", "file:////home/mailtessflo16/chkpt")
 .outputMode("append")
 .start()

val stream2 = nexus_key_val2.writeStream
 .format("kafka")
 .option("kafka.bootstrap.servers", "localhost:9092")
 .option("topic", "idle") .option("checkpointLocation", "file:////home/mailtessflo16/chkpt2")
 .outputMode("append")
 .start()

val stream3 = nexus_key_val3.writeStream
 .format("kafka")
 .option("kafka.bootstrap.servers", "localhost:9092")
 .option("topic", "active") .option("checkpointLocation", "file:////home/mailtessflo16/chkpt3")
 .outputMode("append")
 .start()

val stream4 = nexus_key_val4.writeStream
 .format("kafka")
 .option("kafka.bootstrap.servers", "localhost:9092")
 .option("topic", "active") .option("checkpointLocation", "file:////home/mailtessflo16/chkpt4")
 .outputMode("append")
 .start()
 


